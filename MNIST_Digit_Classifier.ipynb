{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classifier\n",
    "\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.\n",
    "\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset.The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support-vector machine to get an error rate of 0.8%. An extended dataset similar to MNIST called EMNIST has been published in 2017, which contains 240,000 training images, and 40,000 testing images of handwritten digits and characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84780603303407bbd751a1bf3b01c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9883b0d8e0a44d4591040803bb05b627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bc76289e0e41699739e73e6ba79a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a1985d0b034d50960b884dc7a84512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset=MNIST(root='data/', download=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset=MNIST(root='data/', train=False) \n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x1EDC148C448>, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereby, 28x28 Image needs to be converted suitable tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label: ',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Dataset into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=MNIST(root='data/', train=True, transform=transforms.ToTensor()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275, 0.4235, 0.0039, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922, 0.9922, 0.4667, 0.0980,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294, 0.9922, 0.9922, 0.5882,\n",
      "         0.1059],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.3647, 0.9882, 0.9922,\n",
      "         0.7333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765, 0.9922,\n",
      "         0.9765],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098, 0.7176, 0.9922, 0.9922,\n",
      "         0.8118],\n",
      "        [0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922, 0.9922, 0.9922, 0.9804,\n",
      "         0.7137]])\n",
      "Label 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label=dataset[0]\n",
    "print(img_tensor.shape)\n",
    "print(img_tensor[0, 10:20, 10:20])\n",
    "print('Label', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To divide train dataset to training part and validation part\n",
    "\n",
    "Validation dataset taken as 20% of Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_indices(n, val_pct): \n",
    "    n_val=int(val_pct*n)\n",
    "    idxs=np.random.permutation(n)\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices=split_indices(len(dataset), val_pct=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indices), len(val_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load random data into Validation and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "train_sampler=SubsetRandomSampler(train_indices)\n",
    "train_dataset=DataLoader(dataset, batch_size, sampler=train_sampler) \n",
    "val_sampler=SubsetRandomSampler(val_indices)\n",
    "val_dataset=DataLoader(dataset, batch_size, sampler=val_sampler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model=nn.Linear(28*28,10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([[-0.0091, -0.0102,  0.0254,  ...,  0.0332, -0.0205, -0.0097],\n",
      "        [-0.0113, -0.0121, -0.0169,  ...,  0.0211,  0.0160, -0.0179],\n",
      "        [ 0.0195,  0.0125, -0.0081,  ...,  0.0006, -0.0342,  0.0219],\n",
      "        ...,\n",
      "        [-0.0298,  0.0355,  0.0225,  ...,  0.0176, -0.0278,  0.0292],\n",
      "        [ 0.0044, -0.0197, -0.0184,  ..., -0.0315, -0.0294, -0.0338],\n",
      "        [-0.0064, -0.0316,  0.0127,  ...,  0.0216, -0.0008,  0.0241]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0216,  0.0229, -0.0178, -0.0058, -0.0348,  0.0138, -0.0059, -0.0185,\n",
      "         0.0132, -0.0237], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "print(model.bias.shape)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Neural Network to classify Digits into 10 different classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(28*28, 10) \n",
    "    \n",
    "    def forward(self, xb):\n",
    "        xb=xb.reshape(-1, 28*28)\n",
    "        out=self.linear(xb)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Mnist_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0225,  0.0107, -0.0093,  ..., -0.0042, -0.0132,  0.0070],\n",
       "         [-0.0012, -0.0271,  0.0025,  ..., -0.0196, -0.0053, -0.0261],\n",
       "         [ 0.0107,  0.0082,  0.0252,  ..., -0.0171,  0.0136, -0.0248],\n",
       "         ...,\n",
       "         [ 0.0019, -0.0317,  0.0122,  ..., -0.0083, -0.0309, -0.0155],\n",
       "         [-0.0305, -0.0351, -0.0287,  ..., -0.0337, -0.0065,  0.0128],\n",
       "         [-0.0316, -0.0002, -0.0064,  ..., -0.0314,  0.0036, -0.0093]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([ 0.0210, -0.0253, -0.0234,  0.0108,  0.0089,  0.0048,  0.0213,  0.0168,\n",
       "         -0.0177,  0.0228], requires_grad=True)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "tensor([[-0.0269, -0.0519, -0.3173,  0.1428, -0.0749,  0.0368, -0.1152,  0.3728,\n",
      "          0.0209, -0.1736],\n",
      "        [-0.0161, -0.0765, -0.4692,  0.1641,  0.2967,  0.1735,  0.1896,  0.0798,\n",
      "          0.2305, -0.1609],\n",
      "        [-0.4321,  0.3742, -0.1935,  0.0235,  0.2907, -0.2039,  0.1171,  0.4194,\n",
      "         -0.1147, -0.1367],\n",
      "        [-0.2163,  0.0171, -0.0429,  0.1536,  0.1464,  0.3649,  0.2535,  0.0365,\n",
      "          0.0269, -0.1844],\n",
      "        [-0.2009,  0.1894,  0.0241,  0.2980,  0.1637,  0.0192,  0.2249,  0.2456,\n",
      "          0.1119, -0.2298]])\n"
     ]
    }
   ],
   "source": [
    "for im, lb in train_dataset:\n",
    "    outputs=model(im)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(outputs[:5].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert outputs from the model to probabilities signifying weights for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites :  tensor([[0.0976, 0.0952, 0.0730, 0.1156, 0.0930, 0.1040, 0.0893, 0.1455, 0.1024,\n",
      "         0.0843],\n",
      "        [0.0924, 0.0870, 0.0587, 0.1106, 0.1263, 0.1117, 0.1135, 0.1017, 0.1182,\n",
      "         0.0799],\n",
      "        [0.0617, 0.1383, 0.0784, 0.0974, 0.1272, 0.0776, 0.1069, 0.1447, 0.0848,\n",
      "         0.0830]])\n",
      "Sum :  1.0\n"
     ]
    }
   ],
   "source": [
    "prob=F.softmax(outputs, dim=1) \n",
    "print('Probabilites : ', prob[:3].data)\n",
    "print('Sum : ', torch.sum(prob[0]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find class for which the probability is maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 7, 5, 3, 3, 6, 6, 3, 3, 4, 6, 6, 3, 6, 5, 6, 5, 1, 3, 1, 7, 3, 1,\n",
      "        3, 7, 7, 5, 5, 7, 7, 1, 7, 7, 1, 5, 6, 7, 1, 5, 1, 6, 3, 1, 3, 6, 4, 3,\n",
      "        5, 5, 8, 8, 6, 6, 6, 5, 1, 3, 6, 5, 5, 1, 6, 5, 7, 3, 3, 7, 5, 1, 8, 7,\n",
      "        6, 6, 7, 3, 3, 5, 7, 6, 6, 5, 1, 5, 6, 6, 6, 6, 3, 4, 7, 4, 8, 3, 7, 1,\n",
      "        1, 3, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "max_prob, max_ind=torch.max(prob, dim=1) \n",
    "print(max_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 7, 3, 9, 1, 0, 3, 0, 2, 2, 6, 0, 5, 3, 6, 9, 4, 8, 1, 2, 1, 8, 6, 1,\n",
       "        4, 4, 9, 6, 5, 5, 4, 1, 5, 9, 7, 9, 7, 5, 3, 2, 3, 8, 0, 8, 2, 8, 4, 2,\n",
       "        8, 9, 1, 1, 9, 5, 6, 7, 3, 2, 1, 5, 1, 9, 1, 9, 5, 7, 0, 5, 4, 9, 8, 7,\n",
       "        1, 0, 4, 0, 2, 4, 8, 8, 6, 8, 9, 5, 6, 8, 6, 6, 3, 6, 5, 3, 5, 4, 5, 7,\n",
       "        6, 2, 4, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l1,l2): \n",
    "    return torch.sum(l1==l2).item()/len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(lb,max_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function chosen is Softmax with SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=loss_fn(outputs, lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3021, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "optim=torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find loss for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, optim=None, metric=None):\n",
    "    preds=model(xb)\n",
    "    loss=loss_fn(preds, yb)\n",
    "    if optim is not None:\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    metric_result=None\n",
    "    if metric is not None: \n",
    "        metric_result=metric(preds, yb)\n",
    "    return loss.item(), len(xb), metric_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find loss for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, val_dataset, metric=None): \n",
    "    with torch.no_grad():\n",
    "        results=[loss_batch(model, loss_fn, xb,yb,metric=metric) for xb, yb in val_dataset]\n",
    "        losses, nums, metrics=zip(*results)\n",
    "        total=np.sum(nums)\n",
    "        avg_loss=np.sum(np.multiply(losses, nums))/total\n",
    "        avg_metric=None\n",
    "        if metric is not None:\n",
    "            avg_metric=np.sum(np.multiply(metrics, nums))/total\n",
    "        return avg_loss, total, avg_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds=torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds==labels).item()/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3058, Accuracy: 0.1466\n"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc=evaluate(model, loss_fn, val_dataset, metric=accuracy) \n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model and then evaluate on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, optim, train_dl, val_dl, metric=None): \n",
    "    for epoch in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            loss,_,_=loss_batch(model, loss_fn, xb,yb, optim)\n",
    "            \n",
    "        result=evaluate(model, loss_fn, val_dl, metric)\n",
    "        val_loss, total, val_metric=result\n",
    "        \n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, epochs, val_loss))\n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
    "                  .format(epoch+1, epochs, val_loss, metric.__name__, val_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Mnist_Model()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 1.8712, accuracy: 0.6943\n",
      "Epoch [2/1000], Loss: 1.5726, accuracy: 0.7612\n",
      "Epoch [3/1000], Loss: 1.3628, accuracy: 0.7830\n",
      "Epoch [4/1000], Loss: 1.2122, accuracy: 0.7981\n",
      "Epoch [5/1000], Loss: 1.1007, accuracy: 0.8088\n",
      "Epoch [6/1000], Loss: 1.0154, accuracy: 0.8173\n",
      "Epoch [7/1000], Loss: 0.9484, accuracy: 0.8237\n",
      "Epoch [8/1000], Loss: 0.8945, accuracy: 0.8288\n",
      "Epoch [9/1000], Loss: 0.8501, accuracy: 0.8326\n",
      "Epoch [10/1000], Loss: 0.8129, accuracy: 0.8372\n",
      "Epoch [11/1000], Loss: 0.7813, accuracy: 0.8403\n",
      "Epoch [12/1000], Loss: 0.7540, accuracy: 0.8427\n",
      "Epoch [13/1000], Loss: 0.7303, accuracy: 0.8448\n",
      "Epoch [14/1000], Loss: 0.7094, accuracy: 0.8469\n",
      "Epoch [15/1000], Loss: 0.6908, accuracy: 0.8488\n",
      "Epoch [16/1000], Loss: 0.6742, accuracy: 0.8507\n",
      "Epoch [17/1000], Loss: 0.6593, accuracy: 0.8530\n",
      "Epoch [18/1000], Loss: 0.6457, accuracy: 0.8544\n",
      "Epoch [19/1000], Loss: 0.6334, accuracy: 0.8558\n",
      "Epoch [20/1000], Loss: 0.6221, accuracy: 0.8568\n",
      "Epoch [21/1000], Loss: 0.6117, accuracy: 0.8582\n",
      "Epoch [22/1000], Loss: 0.6022, accuracy: 0.8592\n",
      "Epoch [23/1000], Loss: 0.5933, accuracy: 0.8603\n",
      "Epoch [24/1000], Loss: 0.5851, accuracy: 0.8612\n",
      "Epoch [25/1000], Loss: 0.5774, accuracy: 0.8625\n",
      "Epoch [26/1000], Loss: 0.5702, accuracy: 0.8635\n",
      "Epoch [27/1000], Loss: 0.5634, accuracy: 0.8643\n",
      "Epoch [28/1000], Loss: 0.5571, accuracy: 0.8650\n",
      "Epoch [29/1000], Loss: 0.5511, accuracy: 0.8659\n",
      "Epoch [30/1000], Loss: 0.5454, accuracy: 0.8666\n",
      "Epoch [31/1000], Loss: 0.5401, accuracy: 0.8669\n",
      "Epoch [32/1000], Loss: 0.5351, accuracy: 0.8678\n",
      "Epoch [33/1000], Loss: 0.5303, accuracy: 0.8682\n",
      "Epoch [34/1000], Loss: 0.5257, accuracy: 0.8695\n",
      "Epoch [35/1000], Loss: 0.5214, accuracy: 0.8705\n",
      "Epoch [36/1000], Loss: 0.5172, accuracy: 0.8715\n",
      "Epoch [37/1000], Loss: 0.5132, accuracy: 0.8720\n",
      "Epoch [38/1000], Loss: 0.5095, accuracy: 0.8728\n",
      "Epoch [39/1000], Loss: 0.5058, accuracy: 0.8728\n",
      "Epoch [40/1000], Loss: 0.5024, accuracy: 0.8733\n",
      "Epoch [41/1000], Loss: 0.4990, accuracy: 0.8738\n",
      "Epoch [42/1000], Loss: 0.4958, accuracy: 0.8738\n",
      "Epoch [43/1000], Loss: 0.4927, accuracy: 0.8739\n",
      "Epoch [44/1000], Loss: 0.4898, accuracy: 0.8746\n",
      "Epoch [45/1000], Loss: 0.4869, accuracy: 0.8749\n",
      "Epoch [46/1000], Loss: 0.4841, accuracy: 0.8752\n",
      "Epoch [47/1000], Loss: 0.4815, accuracy: 0.8756\n",
      "Epoch [48/1000], Loss: 0.4789, accuracy: 0.8764\n",
      "Epoch [49/1000], Loss: 0.4764, accuracy: 0.8770\n",
      "Epoch [50/1000], Loss: 0.4740, accuracy: 0.8774\n",
      "Epoch [51/1000], Loss: 0.4717, accuracy: 0.8778\n",
      "Epoch [52/1000], Loss: 0.4695, accuracy: 0.8785\n",
      "Epoch [53/1000], Loss: 0.4673, accuracy: 0.8788\n",
      "Epoch [54/1000], Loss: 0.4652, accuracy: 0.8792\n",
      "Epoch [55/1000], Loss: 0.4631, accuracy: 0.8793\n",
      "Epoch [56/1000], Loss: 0.4611, accuracy: 0.8797\n",
      "Epoch [57/1000], Loss: 0.4592, accuracy: 0.8802\n",
      "Epoch [58/1000], Loss: 0.4573, accuracy: 0.8802\n",
      "Epoch [59/1000], Loss: 0.4555, accuracy: 0.8808\n",
      "Epoch [60/1000], Loss: 0.4537, accuracy: 0.8809\n",
      "Epoch [61/1000], Loss: 0.4519, accuracy: 0.8810\n",
      "Epoch [62/1000], Loss: 0.4502, accuracy: 0.8813\n",
      "Epoch [63/1000], Loss: 0.4486, accuracy: 0.8813\n",
      "Epoch [64/1000], Loss: 0.4470, accuracy: 0.8816\n",
      "Epoch [65/1000], Loss: 0.4454, accuracy: 0.8820\n",
      "Epoch [66/1000], Loss: 0.4439, accuracy: 0.8823\n",
      "Epoch [67/1000], Loss: 0.4424, accuracy: 0.8830\n",
      "Epoch [68/1000], Loss: 0.4409, accuracy: 0.8833\n",
      "Epoch [69/1000], Loss: 0.4395, accuracy: 0.8836\n",
      "Epoch [70/1000], Loss: 0.4381, accuracy: 0.8838\n",
      "Epoch [71/1000], Loss: 0.4368, accuracy: 0.8838\n",
      "Epoch [72/1000], Loss: 0.4354, accuracy: 0.8839\n",
      "Epoch [73/1000], Loss: 0.4341, accuracy: 0.8838\n",
      "Epoch [74/1000], Loss: 0.4329, accuracy: 0.8842\n",
      "Epoch [75/1000], Loss: 0.4316, accuracy: 0.8846\n",
      "Epoch [76/1000], Loss: 0.4304, accuracy: 0.8848\n",
      "Epoch [77/1000], Loss: 0.4292, accuracy: 0.8852\n",
      "Epoch [78/1000], Loss: 0.4281, accuracy: 0.8857\n",
      "Epoch [79/1000], Loss: 0.4269, accuracy: 0.8858\n",
      "Epoch [80/1000], Loss: 0.4258, accuracy: 0.8858\n",
      "Epoch [81/1000], Loss: 0.4247, accuracy: 0.8859\n",
      "Epoch [82/1000], Loss: 0.4236, accuracy: 0.8862\n",
      "Epoch [83/1000], Loss: 0.4225, accuracy: 0.8863\n",
      "Epoch [84/1000], Loss: 0.4215, accuracy: 0.8865\n",
      "Epoch [85/1000], Loss: 0.4205, accuracy: 0.8864\n",
      "Epoch [86/1000], Loss: 0.4195, accuracy: 0.8866\n",
      "Epoch [87/1000], Loss: 0.4185, accuracy: 0.8867\n",
      "Epoch [88/1000], Loss: 0.4175, accuracy: 0.8869\n",
      "Epoch [89/1000], Loss: 0.4166, accuracy: 0.8868\n",
      "Epoch [90/1000], Loss: 0.4156, accuracy: 0.8871\n",
      "Epoch [91/1000], Loss: 0.4147, accuracy: 0.8874\n",
      "Epoch [92/1000], Loss: 0.4138, accuracy: 0.8875\n",
      "Epoch [93/1000], Loss: 0.4129, accuracy: 0.8879\n",
      "Epoch [94/1000], Loss: 0.4121, accuracy: 0.8881\n",
      "Epoch [95/1000], Loss: 0.4112, accuracy: 0.8880\n",
      "Epoch [96/1000], Loss: 0.4104, accuracy: 0.8883\n",
      "Epoch [97/1000], Loss: 0.4095, accuracy: 0.8885\n",
      "Epoch [98/1000], Loss: 0.4087, accuracy: 0.8888\n",
      "Epoch [99/1000], Loss: 0.4079, accuracy: 0.8888\n",
      "Epoch [100/1000], Loss: 0.4071, accuracy: 0.8893\n",
      "Epoch [101/1000], Loss: 0.4064, accuracy: 0.8894\n",
      "Epoch [102/1000], Loss: 0.4056, accuracy: 0.8898\n",
      "Epoch [103/1000], Loss: 0.4048, accuracy: 0.8898\n",
      "Epoch [104/1000], Loss: 0.4041, accuracy: 0.8899\n",
      "Epoch [105/1000], Loss: 0.4034, accuracy: 0.8902\n",
      "Epoch [106/1000], Loss: 0.4026, accuracy: 0.8902\n",
      "Epoch [107/1000], Loss: 0.4019, accuracy: 0.8904\n",
      "Epoch [108/1000], Loss: 0.4012, accuracy: 0.8907\n",
      "Epoch [109/1000], Loss: 0.4006, accuracy: 0.8909\n",
      "Epoch [110/1000], Loss: 0.3999, accuracy: 0.8911\n",
      "Epoch [111/1000], Loss: 0.3992, accuracy: 0.8914\n",
      "Epoch [112/1000], Loss: 0.3986, accuracy: 0.8918\n",
      "Epoch [113/1000], Loss: 0.3979, accuracy: 0.8919\n",
      "Epoch [114/1000], Loss: 0.3972, accuracy: 0.8920\n",
      "Epoch [115/1000], Loss: 0.3966, accuracy: 0.8921\n",
      "Epoch [116/1000], Loss: 0.3960, accuracy: 0.8924\n",
      "Epoch [117/1000], Loss: 0.3954, accuracy: 0.8925\n",
      "Epoch [118/1000], Loss: 0.3948, accuracy: 0.8927\n",
      "Epoch [119/1000], Loss: 0.3942, accuracy: 0.8928\n",
      "Epoch [120/1000], Loss: 0.3936, accuracy: 0.8929\n",
      "Epoch [121/1000], Loss: 0.3930, accuracy: 0.8930\n",
      "Epoch [122/1000], Loss: 0.3924, accuracy: 0.8930\n",
      "Epoch [123/1000], Loss: 0.3918, accuracy: 0.8930\n",
      "Epoch [124/1000], Loss: 0.3913, accuracy: 0.8932\n",
      "Epoch [125/1000], Loss: 0.3907, accuracy: 0.8932\n",
      "Epoch [126/1000], Loss: 0.3902, accuracy: 0.8938\n",
      "Epoch [127/1000], Loss: 0.3896, accuracy: 0.8941\n",
      "Epoch [128/1000], Loss: 0.3891, accuracy: 0.8944\n",
      "Epoch [129/1000], Loss: 0.3885, accuracy: 0.8945\n",
      "Epoch [130/1000], Loss: 0.3880, accuracy: 0.8945\n",
      "Epoch [131/1000], Loss: 0.3875, accuracy: 0.8944\n",
      "Epoch [132/1000], Loss: 0.3870, accuracy: 0.8946\n",
      "Epoch [133/1000], Loss: 0.3865, accuracy: 0.8948\n",
      "Epoch [134/1000], Loss: 0.3860, accuracy: 0.8948\n",
      "Epoch [135/1000], Loss: 0.3855, accuracy: 0.8947\n",
      "Epoch [136/1000], Loss: 0.3850, accuracy: 0.8948\n",
      "Epoch [137/1000], Loss: 0.3845, accuracy: 0.8950\n",
      "Epoch [138/1000], Loss: 0.3841, accuracy: 0.8952\n",
      "Epoch [139/1000], Loss: 0.3836, accuracy: 0.8952\n",
      "Epoch [140/1000], Loss: 0.3831, accuracy: 0.8954\n",
      "Epoch [141/1000], Loss: 0.3827, accuracy: 0.8953\n",
      "Epoch [142/1000], Loss: 0.3822, accuracy: 0.8956\n",
      "Epoch [143/1000], Loss: 0.3817, accuracy: 0.8957\n",
      "Epoch [144/1000], Loss: 0.3813, accuracy: 0.8958\n",
      "Epoch [145/1000], Loss: 0.3809, accuracy: 0.8958\n",
      "Epoch [146/1000], Loss: 0.3804, accuracy: 0.8958\n",
      "Epoch [147/1000], Loss: 0.3800, accuracy: 0.8958\n",
      "Epoch [148/1000], Loss: 0.3796, accuracy: 0.8958\n",
      "Epoch [149/1000], Loss: 0.3791, accuracy: 0.8958\n",
      "Epoch [150/1000], Loss: 0.3787, accuracy: 0.8958\n",
      "Epoch [151/1000], Loss: 0.3783, accuracy: 0.8959\n",
      "Epoch [152/1000], Loss: 0.3779, accuracy: 0.8962\n",
      "Epoch [153/1000], Loss: 0.3775, accuracy: 0.8962\n",
      "Epoch [154/1000], Loss: 0.3771, accuracy: 0.8961\n",
      "Epoch [155/1000], Loss: 0.3767, accuracy: 0.8961\n",
      "Epoch [156/1000], Loss: 0.3763, accuracy: 0.8960\n",
      "Epoch [157/1000], Loss: 0.3759, accuracy: 0.8960\n",
      "Epoch [158/1000], Loss: 0.3755, accuracy: 0.8961\n",
      "Epoch [159/1000], Loss: 0.3751, accuracy: 0.8962\n",
      "Epoch [160/1000], Loss: 0.3747, accuracy: 0.8963\n",
      "Epoch [161/1000], Loss: 0.3744, accuracy: 0.8966\n",
      "Epoch [162/1000], Loss: 0.3740, accuracy: 0.8965\n",
      "Epoch [163/1000], Loss: 0.3736, accuracy: 0.8968\n",
      "Epoch [164/1000], Loss: 0.3733, accuracy: 0.8968\n",
      "Epoch [165/1000], Loss: 0.3729, accuracy: 0.8970\n",
      "Epoch [166/1000], Loss: 0.3725, accuracy: 0.8971\n",
      "Epoch [167/1000], Loss: 0.3722, accuracy: 0.8973\n",
      "Epoch [168/1000], Loss: 0.3718, accuracy: 0.8974\n",
      "Epoch [169/1000], Loss: 0.3715, accuracy: 0.8977\n",
      "Epoch [170/1000], Loss: 0.3711, accuracy: 0.8975\n",
      "Epoch [171/1000], Loss: 0.3708, accuracy: 0.8976\n",
      "Epoch [172/1000], Loss: 0.3705, accuracy: 0.8975\n",
      "Epoch [173/1000], Loss: 0.3701, accuracy: 0.8976\n",
      "Epoch [174/1000], Loss: 0.3698, accuracy: 0.8978\n",
      "Epoch [175/1000], Loss: 0.3694, accuracy: 0.8980\n",
      "Epoch [176/1000], Loss: 0.3691, accuracy: 0.8978\n",
      "Epoch [177/1000], Loss: 0.3688, accuracy: 0.8980\n",
      "Epoch [178/1000], Loss: 0.3685, accuracy: 0.8980\n",
      "Epoch [179/1000], Loss: 0.3682, accuracy: 0.8979\n",
      "Epoch [180/1000], Loss: 0.3678, accuracy: 0.8979\n",
      "Epoch [181/1000], Loss: 0.3675, accuracy: 0.8982\n",
      "Epoch [182/1000], Loss: 0.3672, accuracy: 0.8982\n",
      "Epoch [183/1000], Loss: 0.3669, accuracy: 0.8984\n",
      "Epoch [184/1000], Loss: 0.3666, accuracy: 0.8986\n",
      "Epoch [185/1000], Loss: 0.3663, accuracy: 0.8982\n",
      "Epoch [186/1000], Loss: 0.3660, accuracy: 0.8985\n",
      "Epoch [187/1000], Loss: 0.3657, accuracy: 0.8982\n",
      "Epoch [188/1000], Loss: 0.3654, accuracy: 0.8985\n",
      "Epoch [189/1000], Loss: 0.3651, accuracy: 0.8985\n",
      "Epoch [190/1000], Loss: 0.3648, accuracy: 0.8986\n",
      "Epoch [191/1000], Loss: 0.3645, accuracy: 0.8988\n",
      "Epoch [192/1000], Loss: 0.3642, accuracy: 0.8989\n",
      "Epoch [193/1000], Loss: 0.3639, accuracy: 0.8989\n",
      "Epoch [194/1000], Loss: 0.3637, accuracy: 0.8988\n",
      "Epoch [195/1000], Loss: 0.3634, accuracy: 0.8988\n",
      "Epoch [196/1000], Loss: 0.3631, accuracy: 0.8990\n",
      "Epoch [197/1000], Loss: 0.3628, accuracy: 0.8989\n",
      "Epoch [198/1000], Loss: 0.3626, accuracy: 0.8989\n",
      "Epoch [199/1000], Loss: 0.3623, accuracy: 0.8992\n",
      "Epoch [200/1000], Loss: 0.3620, accuracy: 0.8992\n",
      "Epoch [201/1000], Loss: 0.3617, accuracy: 0.8990\n",
      "Epoch [202/1000], Loss: 0.3615, accuracy: 0.8992\n",
      "Epoch [203/1000], Loss: 0.3612, accuracy: 0.8991\n",
      "Epoch [204/1000], Loss: 0.3609, accuracy: 0.8993\n",
      "Epoch [205/1000], Loss: 0.3607, accuracy: 0.8993\n",
      "Epoch [206/1000], Loss: 0.3604, accuracy: 0.8993\n",
      "Epoch [207/1000], Loss: 0.3602, accuracy: 0.8995\n",
      "Epoch [208/1000], Loss: 0.3599, accuracy: 0.8997\n",
      "Epoch [209/1000], Loss: 0.3597, accuracy: 0.8998\n",
      "Epoch [210/1000], Loss: 0.3594, accuracy: 0.9000\n",
      "Epoch [211/1000], Loss: 0.3592, accuracy: 0.8998\n",
      "Epoch [212/1000], Loss: 0.3589, accuracy: 0.9001\n",
      "Epoch [213/1000], Loss: 0.3587, accuracy: 0.9001\n",
      "Epoch [214/1000], Loss: 0.3584, accuracy: 0.9001\n",
      "Epoch [215/1000], Loss: 0.3582, accuracy: 0.9002\n",
      "Epoch [216/1000], Loss: 0.3580, accuracy: 0.9002\n",
      "Epoch [217/1000], Loss: 0.3577, accuracy: 0.9003\n",
      "Epoch [218/1000], Loss: 0.3575, accuracy: 0.9005\n",
      "Epoch [219/1000], Loss: 0.3572, accuracy: 0.9005\n",
      "Epoch [220/1000], Loss: 0.3570, accuracy: 0.9006\n",
      "Epoch [221/1000], Loss: 0.3568, accuracy: 0.9006\n",
      "Epoch [222/1000], Loss: 0.3566, accuracy: 0.9008\n",
      "Epoch [223/1000], Loss: 0.3563, accuracy: 0.9009\n",
      "Epoch [224/1000], Loss: 0.3561, accuracy: 0.9009\n",
      "Epoch [225/1000], Loss: 0.3559, accuracy: 0.9011\n",
      "Epoch [226/1000], Loss: 0.3556, accuracy: 0.9010\n",
      "Epoch [227/1000], Loss: 0.3554, accuracy: 0.9011\n",
      "Epoch [228/1000], Loss: 0.3552, accuracy: 0.9012\n",
      "Epoch [229/1000], Loss: 0.3550, accuracy: 0.9012\n",
      "Epoch [230/1000], Loss: 0.3548, accuracy: 0.9012\n",
      "Epoch [231/1000], Loss: 0.3546, accuracy: 0.9012\n",
      "Epoch [232/1000], Loss: 0.3543, accuracy: 0.9012\n",
      "Epoch [233/1000], Loss: 0.3541, accuracy: 0.9012\n",
      "Epoch [234/1000], Loss: 0.3539, accuracy: 0.9016\n",
      "Epoch [235/1000], Loss: 0.3537, accuracy: 0.9016\n",
      "Epoch [236/1000], Loss: 0.3535, accuracy: 0.9016\n",
      "Epoch [237/1000], Loss: 0.3533, accuracy: 0.9017\n",
      "Epoch [238/1000], Loss: 0.3531, accuracy: 0.9017\n",
      "Epoch [239/1000], Loss: 0.3529, accuracy: 0.9017\n",
      "Epoch [240/1000], Loss: 0.3527, accuracy: 0.9017\n",
      "Epoch [241/1000], Loss: 0.3525, accuracy: 0.9018\n",
      "Epoch [242/1000], Loss: 0.3523, accuracy: 0.9015\n",
      "Epoch [243/1000], Loss: 0.3521, accuracy: 0.9017\n",
      "Epoch [244/1000], Loss: 0.3519, accuracy: 0.9017\n",
      "Epoch [245/1000], Loss: 0.3517, accuracy: 0.9016\n",
      "Epoch [246/1000], Loss: 0.3515, accuracy: 0.9017\n",
      "Epoch [247/1000], Loss: 0.3513, accuracy: 0.9017\n",
      "Epoch [248/1000], Loss: 0.3511, accuracy: 0.9017\n",
      "Epoch [249/1000], Loss: 0.3509, accuracy: 0.9017\n",
      "Epoch [250/1000], Loss: 0.3507, accuracy: 0.9017\n",
      "Epoch [251/1000], Loss: 0.3505, accuracy: 0.9019\n",
      "Epoch [252/1000], Loss: 0.3503, accuracy: 0.9020\n",
      "Epoch [253/1000], Loss: 0.3501, accuracy: 0.9020\n",
      "Epoch [254/1000], Loss: 0.3499, accuracy: 0.9022\n",
      "Epoch [255/1000], Loss: 0.3498, accuracy: 0.9022\n",
      "Epoch [256/1000], Loss: 0.3496, accuracy: 0.9024\n",
      "Epoch [257/1000], Loss: 0.3494, accuracy: 0.9024\n",
      "Epoch [258/1000], Loss: 0.3492, accuracy: 0.9024\n",
      "Epoch [259/1000], Loss: 0.3490, accuracy: 0.9023\n",
      "Epoch [260/1000], Loss: 0.3488, accuracy: 0.9023\n",
      "Epoch [261/1000], Loss: 0.3487, accuracy: 0.9025\n",
      "Epoch [262/1000], Loss: 0.3485, accuracy: 0.9024\n",
      "Epoch [263/1000], Loss: 0.3483, accuracy: 0.9024\n",
      "Epoch [264/1000], Loss: 0.3481, accuracy: 0.9025\n",
      "Epoch [265/1000], Loss: 0.3480, accuracy: 0.9026\n",
      "Epoch [266/1000], Loss: 0.3478, accuracy: 0.9027\n",
      "Epoch [267/1000], Loss: 0.3476, accuracy: 0.9026\n",
      "Epoch [268/1000], Loss: 0.3474, accuracy: 0.9027\n",
      "Epoch [269/1000], Loss: 0.3473, accuracy: 0.9027\n",
      "Epoch [270/1000], Loss: 0.3471, accuracy: 0.9028\n",
      "Epoch [271/1000], Loss: 0.3469, accuracy: 0.9028\n",
      "Epoch [272/1000], Loss: 0.3468, accuracy: 0.9029\n",
      "Epoch [273/1000], Loss: 0.3466, accuracy: 0.9030\n",
      "Epoch [274/1000], Loss: 0.3464, accuracy: 0.9031\n",
      "Epoch [275/1000], Loss: 0.3463, accuracy: 0.9031\n",
      "Epoch [276/1000], Loss: 0.3461, accuracy: 0.9034\n",
      "Epoch [277/1000], Loss: 0.3459, accuracy: 0.9033\n",
      "Epoch [278/1000], Loss: 0.3458, accuracy: 0.9036\n",
      "Epoch [279/1000], Loss: 0.3456, accuracy: 0.9036\n",
      "Epoch [280/1000], Loss: 0.3454, accuracy: 0.9036\n",
      "Epoch [281/1000], Loss: 0.3453, accuracy: 0.9036\n",
      "Epoch [282/1000], Loss: 0.3451, accuracy: 0.9035\n",
      "Epoch [283/1000], Loss: 0.3450, accuracy: 0.9038\n",
      "Epoch [284/1000], Loss: 0.3448, accuracy: 0.9034\n",
      "Epoch [285/1000], Loss: 0.3447, accuracy: 0.9038\n",
      "Epoch [286/1000], Loss: 0.3445, accuracy: 0.9038\n",
      "Epoch [287/1000], Loss: 0.3444, accuracy: 0.9037\n",
      "Epoch [288/1000], Loss: 0.3442, accuracy: 0.9038\n",
      "Epoch [289/1000], Loss: 0.3440, accuracy: 0.9038\n",
      "Epoch [290/1000], Loss: 0.3439, accuracy: 0.9042\n",
      "Epoch [291/1000], Loss: 0.3437, accuracy: 0.9040\n",
      "Epoch [292/1000], Loss: 0.3436, accuracy: 0.9042\n",
      "Epoch [293/1000], Loss: 0.3434, accuracy: 0.9040\n",
      "Epoch [294/1000], Loss: 0.3433, accuracy: 0.9040\n",
      "Epoch [295/1000], Loss: 0.3431, accuracy: 0.9041\n",
      "Epoch [296/1000], Loss: 0.3430, accuracy: 0.9040\n",
      "Epoch [297/1000], Loss: 0.3428, accuracy: 0.9039\n",
      "Epoch [298/1000], Loss: 0.3427, accuracy: 0.9039\n",
      "Epoch [299/1000], Loss: 0.3425, accuracy: 0.9041\n",
      "Epoch [300/1000], Loss: 0.3424, accuracy: 0.9040\n",
      "Epoch [301/1000], Loss: 0.3423, accuracy: 0.9039\n",
      "Epoch [302/1000], Loss: 0.3421, accuracy: 0.9042\n",
      "Epoch [303/1000], Loss: 0.3420, accuracy: 0.9042\n",
      "Epoch [304/1000], Loss: 0.3418, accuracy: 0.9043\n",
      "Epoch [305/1000], Loss: 0.3417, accuracy: 0.9044\n",
      "Epoch [306/1000], Loss: 0.3416, accuracy: 0.9046\n",
      "Epoch [307/1000], Loss: 0.3414, accuracy: 0.9047\n",
      "Epoch [308/1000], Loss: 0.3413, accuracy: 0.9047\n",
      "Epoch [309/1000], Loss: 0.3411, accuracy: 0.9046\n",
      "Epoch [310/1000], Loss: 0.3410, accuracy: 0.9048\n",
      "Epoch [311/1000], Loss: 0.3409, accuracy: 0.9046\n",
      "Epoch [312/1000], Loss: 0.3407, accuracy: 0.9047\n",
      "Epoch [313/1000], Loss: 0.3406, accuracy: 0.9047\n",
      "Epoch [314/1000], Loss: 0.3405, accuracy: 0.9048\n",
      "Epoch [315/1000], Loss: 0.3403, accuracy: 0.9046\n",
      "Epoch [316/1000], Loss: 0.3402, accuracy: 0.9047\n",
      "Epoch [317/1000], Loss: 0.3400, accuracy: 0.9046\n",
      "Epoch [318/1000], Loss: 0.3399, accuracy: 0.9046\n",
      "Epoch [319/1000], Loss: 0.3398, accuracy: 0.9046\n",
      "Epoch [320/1000], Loss: 0.3396, accuracy: 0.9047\n",
      "Epoch [321/1000], Loss: 0.3395, accuracy: 0.9046\n",
      "Epoch [322/1000], Loss: 0.3394, accuracy: 0.9046\n",
      "Epoch [323/1000], Loss: 0.3393, accuracy: 0.9048\n",
      "Epoch [324/1000], Loss: 0.3391, accuracy: 0.9048\n",
      "Epoch [325/1000], Loss: 0.3390, accuracy: 0.9052\n",
      "Epoch [326/1000], Loss: 0.3389, accuracy: 0.9052\n",
      "Epoch [327/1000], Loss: 0.3388, accuracy: 0.9052\n",
      "Epoch [328/1000], Loss: 0.3386, accuracy: 0.9053\n",
      "Epoch [329/1000], Loss: 0.3385, accuracy: 0.9051\n",
      "Epoch [330/1000], Loss: 0.3384, accuracy: 0.9051\n",
      "Epoch [331/1000], Loss: 0.3383, accuracy: 0.9051\n",
      "Epoch [332/1000], Loss: 0.3381, accuracy: 0.9054\n",
      "Epoch [333/1000], Loss: 0.3380, accuracy: 0.9052\n",
      "Epoch [334/1000], Loss: 0.3379, accuracy: 0.9053\n",
      "Epoch [335/1000], Loss: 0.3378, accuracy: 0.9053\n",
      "Epoch [336/1000], Loss: 0.3376, accuracy: 0.9054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [337/1000], Loss: 0.3375, accuracy: 0.9056\n",
      "Epoch [338/1000], Loss: 0.3374, accuracy: 0.9056\n",
      "Epoch [339/1000], Loss: 0.3373, accuracy: 0.9058\n",
      "Epoch [340/1000], Loss: 0.3372, accuracy: 0.9059\n",
      "Epoch [341/1000], Loss: 0.3370, accuracy: 0.9058\n",
      "Epoch [342/1000], Loss: 0.3369, accuracy: 0.9058\n",
      "Epoch [343/1000], Loss: 0.3368, accuracy: 0.9058\n",
      "Epoch [344/1000], Loss: 0.3367, accuracy: 0.9060\n",
      "Epoch [345/1000], Loss: 0.3366, accuracy: 0.9058\n",
      "Epoch [346/1000], Loss: 0.3365, accuracy: 0.9059\n",
      "Epoch [347/1000], Loss: 0.3363, accuracy: 0.9058\n",
      "Epoch [348/1000], Loss: 0.3362, accuracy: 0.9060\n",
      "Epoch [349/1000], Loss: 0.3361, accuracy: 0.9062\n",
      "Epoch [350/1000], Loss: 0.3360, accuracy: 0.9062\n",
      "Epoch [351/1000], Loss: 0.3359, accuracy: 0.9062\n",
      "Epoch [352/1000], Loss: 0.3358, accuracy: 0.9062\n",
      "Epoch [353/1000], Loss: 0.3357, accuracy: 0.9062\n",
      "Epoch [354/1000], Loss: 0.3355, accuracy: 0.9064\n",
      "Epoch [355/1000], Loss: 0.3354, accuracy: 0.9064\n",
      "Epoch [356/1000], Loss: 0.3353, accuracy: 0.9064\n",
      "Epoch [357/1000], Loss: 0.3352, accuracy: 0.9065\n",
      "Epoch [358/1000], Loss: 0.3351, accuracy: 0.9067\n",
      "Epoch [359/1000], Loss: 0.3350, accuracy: 0.9067\n",
      "Epoch [360/1000], Loss: 0.3349, accuracy: 0.9067\n",
      "Epoch [361/1000], Loss: 0.3348, accuracy: 0.9067\n",
      "Epoch [362/1000], Loss: 0.3347, accuracy: 0.9067\n",
      "Epoch [363/1000], Loss: 0.3346, accuracy: 0.9069\n",
      "Epoch [364/1000], Loss: 0.3345, accuracy: 0.9069\n",
      "Epoch [365/1000], Loss: 0.3343, accuracy: 0.9068\n",
      "Epoch [366/1000], Loss: 0.3342, accuracy: 0.9071\n",
      "Epoch [367/1000], Loss: 0.3341, accuracy: 0.9071\n",
      "Epoch [368/1000], Loss: 0.3340, accuracy: 0.9069\n",
      "Epoch [369/1000], Loss: 0.3339, accuracy: 0.9071\n",
      "Epoch [370/1000], Loss: 0.3338, accuracy: 0.9071\n",
      "Epoch [371/1000], Loss: 0.3337, accuracy: 0.9071\n",
      "Epoch [372/1000], Loss: 0.3336, accuracy: 0.9072\n",
      "Epoch [373/1000], Loss: 0.3335, accuracy: 0.9072\n",
      "Epoch [374/1000], Loss: 0.3334, accuracy: 0.9072\n",
      "Epoch [375/1000], Loss: 0.3333, accuracy: 0.9071\n",
      "Epoch [376/1000], Loss: 0.3332, accuracy: 0.9071\n",
      "Epoch [377/1000], Loss: 0.3331, accuracy: 0.9071\n",
      "Epoch [378/1000], Loss: 0.3330, accuracy: 0.9071\n",
      "Epoch [379/1000], Loss: 0.3329, accuracy: 0.9073\n",
      "Epoch [380/1000], Loss: 0.3328, accuracy: 0.9074\n",
      "Epoch [381/1000], Loss: 0.3327, accuracy: 0.9073\n",
      "Epoch [382/1000], Loss: 0.3326, accuracy: 0.9074\n",
      "Epoch [383/1000], Loss: 0.3325, accuracy: 0.9075\n",
      "Epoch [384/1000], Loss: 0.3324, accuracy: 0.9073\n",
      "Epoch [385/1000], Loss: 0.3323, accuracy: 0.9073\n",
      "Epoch [386/1000], Loss: 0.3322, accuracy: 0.9075\n",
      "Epoch [387/1000], Loss: 0.3321, accuracy: 0.9074\n",
      "Epoch [388/1000], Loss: 0.3320, accuracy: 0.9075\n",
      "Epoch [389/1000], Loss: 0.3319, accuracy: 0.9073\n",
      "Epoch [390/1000], Loss: 0.3318, accuracy: 0.9073\n",
      "Epoch [391/1000], Loss: 0.3317, accuracy: 0.9073\n",
      "Epoch [392/1000], Loss: 0.3316, accuracy: 0.9076\n",
      "Epoch [393/1000], Loss: 0.3315, accuracy: 0.9077\n",
      "Epoch [394/1000], Loss: 0.3314, accuracy: 0.9080\n",
      "Epoch [395/1000], Loss: 0.3313, accuracy: 0.9078\n",
      "Epoch [396/1000], Loss: 0.3312, accuracy: 0.9077\n",
      "Epoch [397/1000], Loss: 0.3311, accuracy: 0.9081\n",
      "Epoch [398/1000], Loss: 0.3310, accuracy: 0.9081\n",
      "Epoch [399/1000], Loss: 0.3310, accuracy: 0.9080\n",
      "Epoch [400/1000], Loss: 0.3309, accuracy: 0.9080\n",
      "Epoch [401/1000], Loss: 0.3308, accuracy: 0.9081\n",
      "Epoch [402/1000], Loss: 0.3307, accuracy: 0.9080\n",
      "Epoch [403/1000], Loss: 0.3306, accuracy: 0.9080\n",
      "Epoch [404/1000], Loss: 0.3305, accuracy: 0.9080\n",
      "Epoch [405/1000], Loss: 0.3304, accuracy: 0.9081\n",
      "Epoch [406/1000], Loss: 0.3303, accuracy: 0.9082\n",
      "Epoch [407/1000], Loss: 0.3302, accuracy: 0.9079\n",
      "Epoch [408/1000], Loss: 0.3301, accuracy: 0.9079\n",
      "Epoch [409/1000], Loss: 0.3300, accuracy: 0.9078\n",
      "Epoch [410/1000], Loss: 0.3300, accuracy: 0.9080\n",
      "Epoch [411/1000], Loss: 0.3299, accuracy: 0.9081\n",
      "Epoch [412/1000], Loss: 0.3298, accuracy: 0.9080\n",
      "Epoch [413/1000], Loss: 0.3297, accuracy: 0.9079\n",
      "Epoch [414/1000], Loss: 0.3296, accuracy: 0.9080\n",
      "Epoch [415/1000], Loss: 0.3295, accuracy: 0.9081\n",
      "Epoch [416/1000], Loss: 0.3294, accuracy: 0.9080\n",
      "Epoch [417/1000], Loss: 0.3293, accuracy: 0.9080\n",
      "Epoch [418/1000], Loss: 0.3293, accuracy: 0.9080\n",
      "Epoch [419/1000], Loss: 0.3292, accuracy: 0.9079\n",
      "Epoch [420/1000], Loss: 0.3291, accuracy: 0.9079\n",
      "Epoch [421/1000], Loss: 0.3290, accuracy: 0.9081\n",
      "Epoch [422/1000], Loss: 0.3289, accuracy: 0.9082\n",
      "Epoch [423/1000], Loss: 0.3288, accuracy: 0.9082\n",
      "Epoch [424/1000], Loss: 0.3287, accuracy: 0.9081\n",
      "Epoch [425/1000], Loss: 0.3286, accuracy: 0.9082\n",
      "Epoch [426/1000], Loss: 0.3286, accuracy: 0.9081\n",
      "Epoch [427/1000], Loss: 0.3285, accuracy: 0.9082\n",
      "Epoch [428/1000], Loss: 0.3284, accuracy: 0.9082\n",
      "Epoch [429/1000], Loss: 0.3283, accuracy: 0.9081\n",
      "Epoch [430/1000], Loss: 0.3282, accuracy: 0.9081\n",
      "Epoch [431/1000], Loss: 0.3282, accuracy: 0.9082\n",
      "Epoch [432/1000], Loss: 0.3281, accuracy: 0.9083\n",
      "Epoch [433/1000], Loss: 0.3280, accuracy: 0.9081\n",
      "Epoch [434/1000], Loss: 0.3279, accuracy: 0.9082\n",
      "Epoch [435/1000], Loss: 0.3278, accuracy: 0.9082\n",
      "Epoch [436/1000], Loss: 0.3277, accuracy: 0.9083\n",
      "Epoch [437/1000], Loss: 0.3277, accuracy: 0.9082\n",
      "Epoch [438/1000], Loss: 0.3276, accuracy: 0.9082\n",
      "Epoch [439/1000], Loss: 0.3275, accuracy: 0.9083\n",
      "Epoch [440/1000], Loss: 0.3274, accuracy: 0.9083\n",
      "Epoch [441/1000], Loss: 0.3273, accuracy: 0.9083\n",
      "Epoch [442/1000], Loss: 0.3273, accuracy: 0.9083\n",
      "Epoch [443/1000], Loss: 0.3272, accuracy: 0.9084\n",
      "Epoch [444/1000], Loss: 0.3271, accuracy: 0.9083\n",
      "Epoch [445/1000], Loss: 0.3270, accuracy: 0.9083\n",
      "Epoch [446/1000], Loss: 0.3270, accuracy: 0.9084\n",
      "Epoch [447/1000], Loss: 0.3269, accuracy: 0.9085\n",
      "Epoch [448/1000], Loss: 0.3268, accuracy: 0.9083\n",
      "Epoch [449/1000], Loss: 0.3267, accuracy: 0.9084\n",
      "Epoch [450/1000], Loss: 0.3266, accuracy: 0.9085\n",
      "Epoch [451/1000], Loss: 0.3266, accuracy: 0.9084\n",
      "Epoch [452/1000], Loss: 0.3265, accuracy: 0.9086\n",
      "Epoch [453/1000], Loss: 0.3264, accuracy: 0.9085\n",
      "Epoch [454/1000], Loss: 0.3263, accuracy: 0.9086\n",
      "Epoch [455/1000], Loss: 0.3262, accuracy: 0.9086\n",
      "Epoch [456/1000], Loss: 0.3262, accuracy: 0.9086\n",
      "Epoch [457/1000], Loss: 0.3261, accuracy: 0.9086\n",
      "Epoch [458/1000], Loss: 0.3260, accuracy: 0.9086\n",
      "Epoch [459/1000], Loss: 0.3259, accuracy: 0.9087\n",
      "Epoch [460/1000], Loss: 0.3259, accuracy: 0.9087\n",
      "Epoch [461/1000], Loss: 0.3258, accuracy: 0.9087\n",
      "Epoch [462/1000], Loss: 0.3257, accuracy: 0.9087\n",
      "Epoch [463/1000], Loss: 0.3257, accuracy: 0.9087\n",
      "Epoch [464/1000], Loss: 0.3256, accuracy: 0.9087\n",
      "Epoch [465/1000], Loss: 0.3255, accuracy: 0.9087\n",
      "Epoch [466/1000], Loss: 0.3254, accuracy: 0.9087\n",
      "Epoch [467/1000], Loss: 0.3254, accuracy: 0.9087\n",
      "Epoch [468/1000], Loss: 0.3253, accuracy: 0.9087\n",
      "Epoch [469/1000], Loss: 0.3252, accuracy: 0.9087\n",
      "Epoch [470/1000], Loss: 0.3252, accuracy: 0.9087\n",
      "Epoch [471/1000], Loss: 0.3251, accuracy: 0.9087\n",
      "Epoch [472/1000], Loss: 0.3250, accuracy: 0.9086\n",
      "Epoch [473/1000], Loss: 0.3249, accuracy: 0.9087\n",
      "Epoch [474/1000], Loss: 0.3249, accuracy: 0.9087\n",
      "Epoch [475/1000], Loss: 0.3248, accuracy: 0.9087\n",
      "Epoch [476/1000], Loss: 0.3247, accuracy: 0.9087\n",
      "Epoch [477/1000], Loss: 0.3246, accuracy: 0.9087\n",
      "Epoch [478/1000], Loss: 0.3246, accuracy: 0.9087\n",
      "Epoch [479/1000], Loss: 0.3245, accuracy: 0.9087\n",
      "Epoch [480/1000], Loss: 0.3244, accuracy: 0.9087\n",
      "Epoch [481/1000], Loss: 0.3244, accuracy: 0.9087\n",
      "Epoch [482/1000], Loss: 0.3243, accuracy: 0.9087\n",
      "Epoch [483/1000], Loss: 0.3242, accuracy: 0.9087\n",
      "Epoch [484/1000], Loss: 0.3242, accuracy: 0.9087\n",
      "Epoch [485/1000], Loss: 0.3241, accuracy: 0.9087\n",
      "Epoch [486/1000], Loss: 0.3240, accuracy: 0.9087\n",
      "Epoch [487/1000], Loss: 0.3240, accuracy: 0.9087\n",
      "Epoch [488/1000], Loss: 0.3239, accuracy: 0.9087\n",
      "Epoch [489/1000], Loss: 0.3238, accuracy: 0.9087\n",
      "Epoch [490/1000], Loss: 0.3237, accuracy: 0.9087\n",
      "Epoch [491/1000], Loss: 0.3237, accuracy: 0.9087\n",
      "Epoch [492/1000], Loss: 0.3236, accuracy: 0.9087\n",
      "Epoch [493/1000], Loss: 0.3236, accuracy: 0.9087\n",
      "Epoch [494/1000], Loss: 0.3235, accuracy: 0.9087\n",
      "Epoch [495/1000], Loss: 0.3234, accuracy: 0.9088\n",
      "Epoch [496/1000], Loss: 0.3234, accuracy: 0.9087\n",
      "Epoch [497/1000], Loss: 0.3233, accuracy: 0.9090\n",
      "Epoch [498/1000], Loss: 0.3232, accuracy: 0.9089\n",
      "Epoch [499/1000], Loss: 0.3232, accuracy: 0.9089\n",
      "Epoch [500/1000], Loss: 0.3231, accuracy: 0.9089\n",
      "Epoch [501/1000], Loss: 0.3230, accuracy: 0.9088\n",
      "Epoch [502/1000], Loss: 0.3230, accuracy: 0.9089\n",
      "Epoch [503/1000], Loss: 0.3229, accuracy: 0.9091\n",
      "Epoch [504/1000], Loss: 0.3228, accuracy: 0.9090\n",
      "Epoch [505/1000], Loss: 0.3228, accuracy: 0.9090\n",
      "Epoch [506/1000], Loss: 0.3227, accuracy: 0.9090\n",
      "Epoch [507/1000], Loss: 0.3226, accuracy: 0.9090\n",
      "Epoch [508/1000], Loss: 0.3226, accuracy: 0.9090\n",
      "Epoch [509/1000], Loss: 0.3225, accuracy: 0.9089\n",
      "Epoch [510/1000], Loss: 0.3224, accuracy: 0.9091\n",
      "Epoch [511/1000], Loss: 0.3224, accuracy: 0.9092\n",
      "Epoch [512/1000], Loss: 0.3223, accuracy: 0.9091\n",
      "Epoch [513/1000], Loss: 0.3223, accuracy: 0.9092\n",
      "Epoch [514/1000], Loss: 0.3222, accuracy: 0.9092\n",
      "Epoch [515/1000], Loss: 0.3221, accuracy: 0.9093\n",
      "Epoch [516/1000], Loss: 0.3221, accuracy: 0.9093\n",
      "Epoch [517/1000], Loss: 0.3220, accuracy: 0.9093\n",
      "Epoch [518/1000], Loss: 0.3219, accuracy: 0.9094\n",
      "Epoch [519/1000], Loss: 0.3219, accuracy: 0.9094\n",
      "Epoch [520/1000], Loss: 0.3218, accuracy: 0.9094\n",
      "Epoch [521/1000], Loss: 0.3218, accuracy: 0.9094\n",
      "Epoch [522/1000], Loss: 0.3217, accuracy: 0.9095\n",
      "Epoch [523/1000], Loss: 0.3216, accuracy: 0.9095\n",
      "Epoch [524/1000], Loss: 0.3216, accuracy: 0.9095\n",
      "Epoch [525/1000], Loss: 0.3215, accuracy: 0.9093\n",
      "Epoch [526/1000], Loss: 0.3215, accuracy: 0.9094\n",
      "Epoch [527/1000], Loss: 0.3214, accuracy: 0.9093\n",
      "Epoch [528/1000], Loss: 0.3213, accuracy: 0.9093\n",
      "Epoch [529/1000], Loss: 0.3213, accuracy: 0.9093\n",
      "Epoch [530/1000], Loss: 0.3212, accuracy: 0.9093\n",
      "Epoch [531/1000], Loss: 0.3212, accuracy: 0.9094\n",
      "Epoch [532/1000], Loss: 0.3211, accuracy: 0.9093\n",
      "Epoch [533/1000], Loss: 0.3210, accuracy: 0.9094\n",
      "Epoch [534/1000], Loss: 0.3210, accuracy: 0.9093\n",
      "Epoch [535/1000], Loss: 0.3209, accuracy: 0.9093\n",
      "Epoch [536/1000], Loss: 0.3209, accuracy: 0.9094\n",
      "Epoch [537/1000], Loss: 0.3208, accuracy: 0.9095\n",
      "Epoch [538/1000], Loss: 0.3207, accuracy: 0.9095\n",
      "Epoch [539/1000], Loss: 0.3207, accuracy: 0.9096\n",
      "Epoch [540/1000], Loss: 0.3206, accuracy: 0.9095\n",
      "Epoch [541/1000], Loss: 0.3206, accuracy: 0.9095\n",
      "Epoch [542/1000], Loss: 0.3205, accuracy: 0.9096\n",
      "Epoch [543/1000], Loss: 0.3205, accuracy: 0.9095\n",
      "Epoch [544/1000], Loss: 0.3204, accuracy: 0.9097\n",
      "Epoch [545/1000], Loss: 0.3203, accuracy: 0.9097\n",
      "Epoch [546/1000], Loss: 0.3203, accuracy: 0.9097\n",
      "Epoch [547/1000], Loss: 0.3202, accuracy: 0.9097\n",
      "Epoch [548/1000], Loss: 0.3202, accuracy: 0.9096\n",
      "Epoch [549/1000], Loss: 0.3201, accuracy: 0.9096\n",
      "Epoch [550/1000], Loss: 0.3201, accuracy: 0.9097\n",
      "Epoch [551/1000], Loss: 0.3200, accuracy: 0.9096\n",
      "Epoch [552/1000], Loss: 0.3199, accuracy: 0.9097\n",
      "Epoch [553/1000], Loss: 0.3199, accuracy: 0.9096\n",
      "Epoch [554/1000], Loss: 0.3198, accuracy: 0.9097\n",
      "Epoch [555/1000], Loss: 0.3198, accuracy: 0.9097\n",
      "Epoch [556/1000], Loss: 0.3197, accuracy: 0.9097\n",
      "Epoch [557/1000], Loss: 0.3197, accuracy: 0.9097\n",
      "Epoch [558/1000], Loss: 0.3196, accuracy: 0.9097\n",
      "Epoch [559/1000], Loss: 0.3196, accuracy: 0.9097\n",
      "Epoch [560/1000], Loss: 0.3195, accuracy: 0.9097\n",
      "Epoch [561/1000], Loss: 0.3194, accuracy: 0.9097\n",
      "Epoch [562/1000], Loss: 0.3194, accuracy: 0.9097\n",
      "Epoch [563/1000], Loss: 0.3193, accuracy: 0.9098\n",
      "Epoch [564/1000], Loss: 0.3193, accuracy: 0.9098\n",
      "Epoch [565/1000], Loss: 0.3192, accuracy: 0.9097\n",
      "Epoch [566/1000], Loss: 0.3192, accuracy: 0.9097\n",
      "Epoch [567/1000], Loss: 0.3191, accuracy: 0.9098\n",
      "Epoch [568/1000], Loss: 0.3191, accuracy: 0.9098\n",
      "Epoch [569/1000], Loss: 0.3190, accuracy: 0.9100\n",
      "Epoch [570/1000], Loss: 0.3190, accuracy: 0.9098\n",
      "Epoch [571/1000], Loss: 0.3189, accuracy: 0.9099\n",
      "Epoch [572/1000], Loss: 0.3189, accuracy: 0.9101\n",
      "Epoch [573/1000], Loss: 0.3188, accuracy: 0.9102\n",
      "Epoch [574/1000], Loss: 0.3187, accuracy: 0.9102\n",
      "Epoch [575/1000], Loss: 0.3187, accuracy: 0.9102\n",
      "Epoch [576/1000], Loss: 0.3186, accuracy: 0.9102\n",
      "Epoch [577/1000], Loss: 0.3186, accuracy: 0.9102\n",
      "Epoch [578/1000], Loss: 0.3185, accuracy: 0.9103\n",
      "Epoch [579/1000], Loss: 0.3185, accuracy: 0.9103\n",
      "Epoch [580/1000], Loss: 0.3184, accuracy: 0.9103\n",
      "Epoch [581/1000], Loss: 0.3184, accuracy: 0.9103\n",
      "Epoch [582/1000], Loss: 0.3183, accuracy: 0.9103\n",
      "Epoch [583/1000], Loss: 0.3183, accuracy: 0.9103\n",
      "Epoch [584/1000], Loss: 0.3182, accuracy: 0.9103\n",
      "Epoch [585/1000], Loss: 0.3182, accuracy: 0.9103\n",
      "Epoch [586/1000], Loss: 0.3181, accuracy: 0.9104\n",
      "Epoch [587/1000], Loss: 0.3181, accuracy: 0.9104\n",
      "Epoch [588/1000], Loss: 0.3180, accuracy: 0.9104\n",
      "Epoch [589/1000], Loss: 0.3180, accuracy: 0.9104\n",
      "Epoch [590/1000], Loss: 0.3179, accuracy: 0.9104\n",
      "Epoch [591/1000], Loss: 0.3179, accuracy: 0.9104\n",
      "Epoch [592/1000], Loss: 0.3178, accuracy: 0.9104\n",
      "Epoch [593/1000], Loss: 0.3178, accuracy: 0.9103\n",
      "Epoch [594/1000], Loss: 0.3177, accuracy: 0.9104\n",
      "Epoch [595/1000], Loss: 0.3177, accuracy: 0.9103\n",
      "Epoch [596/1000], Loss: 0.3176, accuracy: 0.9104\n",
      "Epoch [597/1000], Loss: 0.3176, accuracy: 0.9104\n",
      "Epoch [598/1000], Loss: 0.3175, accuracy: 0.9104\n",
      "Epoch [599/1000], Loss: 0.3175, accuracy: 0.9103\n",
      "Epoch [600/1000], Loss: 0.3174, accuracy: 0.9103\n",
      "Epoch [601/1000], Loss: 0.3174, accuracy: 0.9104\n",
      "Epoch [602/1000], Loss: 0.3173, accuracy: 0.9104\n",
      "Epoch [603/1000], Loss: 0.3173, accuracy: 0.9107\n",
      "Epoch [604/1000], Loss: 0.3172, accuracy: 0.9107\n",
      "Epoch [605/1000], Loss: 0.3172, accuracy: 0.9105\n",
      "Epoch [606/1000], Loss: 0.3171, accuracy: 0.9106\n",
      "Epoch [607/1000], Loss: 0.3171, accuracy: 0.9107\n",
      "Epoch [608/1000], Loss: 0.3170, accuracy: 0.9107\n",
      "Epoch [609/1000], Loss: 0.3170, accuracy: 0.9105\n",
      "Epoch [610/1000], Loss: 0.3169, accuracy: 0.9106\n",
      "Epoch [611/1000], Loss: 0.3169, accuracy: 0.9106\n",
      "Epoch [612/1000], Loss: 0.3169, accuracy: 0.9106\n",
      "Epoch [613/1000], Loss: 0.3168, accuracy: 0.9107\n",
      "Epoch [614/1000], Loss: 0.3168, accuracy: 0.9107\n",
      "Epoch [615/1000], Loss: 0.3167, accuracy: 0.9107\n",
      "Epoch [616/1000], Loss: 0.3167, accuracy: 0.9107\n",
      "Epoch [617/1000], Loss: 0.3166, accuracy: 0.9107\n",
      "Epoch [618/1000], Loss: 0.3166, accuracy: 0.9107\n",
      "Epoch [619/1000], Loss: 0.3165, accuracy: 0.9108\n",
      "Epoch [620/1000], Loss: 0.3165, accuracy: 0.9108\n",
      "Epoch [621/1000], Loss: 0.3164, accuracy: 0.9108\n",
      "Epoch [622/1000], Loss: 0.3164, accuracy: 0.9107\n",
      "Epoch [623/1000], Loss: 0.3163, accuracy: 0.9108\n",
      "Epoch [624/1000], Loss: 0.3163, accuracy: 0.9107\n",
      "Epoch [625/1000], Loss: 0.3162, accuracy: 0.9106\n",
      "Epoch [626/1000], Loss: 0.3162, accuracy: 0.9106\n",
      "Epoch [627/1000], Loss: 0.3162, accuracy: 0.9106\n",
      "Epoch [628/1000], Loss: 0.3161, accuracy: 0.9107\n",
      "Epoch [629/1000], Loss: 0.3161, accuracy: 0.9106\n",
      "Epoch [630/1000], Loss: 0.3160, accuracy: 0.9107\n",
      "Epoch [631/1000], Loss: 0.3160, accuracy: 0.9105\n",
      "Epoch [632/1000], Loss: 0.3159, accuracy: 0.9107\n",
      "Epoch [633/1000], Loss: 0.3159, accuracy: 0.9107\n",
      "Epoch [634/1000], Loss: 0.3158, accuracy: 0.9107\n",
      "Epoch [635/1000], Loss: 0.3158, accuracy: 0.9107\n",
      "Epoch [636/1000], Loss: 0.3157, accuracy: 0.9107\n",
      "Epoch [637/1000], Loss: 0.3157, accuracy: 0.9107\n",
      "Epoch [638/1000], Loss: 0.3157, accuracy: 0.9107\n",
      "Epoch [639/1000], Loss: 0.3156, accuracy: 0.9107\n",
      "Epoch [640/1000], Loss: 0.3156, accuracy: 0.9107\n",
      "Epoch [641/1000], Loss: 0.3155, accuracy: 0.9107\n",
      "Epoch [642/1000], Loss: 0.3155, accuracy: 0.9107\n",
      "Epoch [643/1000], Loss: 0.3154, accuracy: 0.9107\n",
      "Epoch [644/1000], Loss: 0.3154, accuracy: 0.9107\n",
      "Epoch [645/1000], Loss: 0.3154, accuracy: 0.9109\n",
      "Epoch [646/1000], Loss: 0.3153, accuracy: 0.9111\n",
      "Epoch [647/1000], Loss: 0.3153, accuracy: 0.9111\n",
      "Epoch [648/1000], Loss: 0.3152, accuracy: 0.9111\n",
      "Epoch [649/1000], Loss: 0.3152, accuracy: 0.9110\n",
      "Epoch [650/1000], Loss: 0.3151, accuracy: 0.9110\n",
      "Epoch [651/1000], Loss: 0.3151, accuracy: 0.9111\n",
      "Epoch [652/1000], Loss: 0.3150, accuracy: 0.9113\n",
      "Epoch [653/1000], Loss: 0.3150, accuracy: 0.9111\n",
      "Epoch [654/1000], Loss: 0.3150, accuracy: 0.9113\n",
      "Epoch [655/1000], Loss: 0.3149, accuracy: 0.9113\n",
      "Epoch [656/1000], Loss: 0.3149, accuracy: 0.9113\n",
      "Epoch [657/1000], Loss: 0.3148, accuracy: 0.9113\n",
      "Epoch [658/1000], Loss: 0.3148, accuracy: 0.9113\n",
      "Epoch [659/1000], Loss: 0.3148, accuracy: 0.9111\n",
      "Epoch [660/1000], Loss: 0.3147, accuracy: 0.9111\n",
      "Epoch [661/1000], Loss: 0.3147, accuracy: 0.9112\n",
      "Epoch [662/1000], Loss: 0.3146, accuracy: 0.9112\n",
      "Epoch [663/1000], Loss: 0.3146, accuracy: 0.9112\n",
      "Epoch [664/1000], Loss: 0.3145, accuracy: 0.9113\n",
      "Epoch [665/1000], Loss: 0.3145, accuracy: 0.9113\n",
      "Epoch [666/1000], Loss: 0.3145, accuracy: 0.9113\n",
      "Epoch [667/1000], Loss: 0.3144, accuracy: 0.9113\n",
      "Epoch [668/1000], Loss: 0.3144, accuracy: 0.9113\n",
      "Epoch [669/1000], Loss: 0.3143, accuracy: 0.9113\n",
      "Epoch [670/1000], Loss: 0.3143, accuracy: 0.9115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [671/1000], Loss: 0.3143, accuracy: 0.9115\n",
      "Epoch [672/1000], Loss: 0.3142, accuracy: 0.9115\n",
      "Epoch [673/1000], Loss: 0.3142, accuracy: 0.9115\n",
      "Epoch [674/1000], Loss: 0.3141, accuracy: 0.9115\n",
      "Epoch [675/1000], Loss: 0.3141, accuracy: 0.9116\n",
      "Epoch [676/1000], Loss: 0.3140, accuracy: 0.9117\n",
      "Epoch [677/1000], Loss: 0.3140, accuracy: 0.9118\n",
      "Epoch [678/1000], Loss: 0.3140, accuracy: 0.9118\n",
      "Epoch [679/1000], Loss: 0.3139, accuracy: 0.9119\n",
      "Epoch [680/1000], Loss: 0.3139, accuracy: 0.9119\n",
      "Epoch [681/1000], Loss: 0.3138, accuracy: 0.9118\n",
      "Epoch [682/1000], Loss: 0.3138, accuracy: 0.9119\n",
      "Epoch [683/1000], Loss: 0.3138, accuracy: 0.9120\n",
      "Epoch [684/1000], Loss: 0.3137, accuracy: 0.9120\n",
      "Epoch [685/1000], Loss: 0.3137, accuracy: 0.9119\n",
      "Epoch [686/1000], Loss: 0.3136, accuracy: 0.9120\n",
      "Epoch [687/1000], Loss: 0.3136, accuracy: 0.9119\n",
      "Epoch [688/1000], Loss: 0.3136, accuracy: 0.9119\n",
      "Epoch [689/1000], Loss: 0.3135, accuracy: 0.9119\n",
      "Epoch [690/1000], Loss: 0.3135, accuracy: 0.9119\n",
      "Epoch [691/1000], Loss: 0.3134, accuracy: 0.9119\n",
      "Epoch [692/1000], Loss: 0.3134, accuracy: 0.9120\n",
      "Epoch [693/1000], Loss: 0.3134, accuracy: 0.9118\n",
      "Epoch [694/1000], Loss: 0.3133, accuracy: 0.9120\n",
      "Epoch [695/1000], Loss: 0.3133, accuracy: 0.9120\n",
      "Epoch [696/1000], Loss: 0.3132, accuracy: 0.9120\n",
      "Epoch [697/1000], Loss: 0.3132, accuracy: 0.9119\n",
      "Epoch [698/1000], Loss: 0.3132, accuracy: 0.9119\n",
      "Epoch [699/1000], Loss: 0.3131, accuracy: 0.9119\n",
      "Epoch [700/1000], Loss: 0.3131, accuracy: 0.9120\n",
      "Epoch [701/1000], Loss: 0.3131, accuracy: 0.9119\n",
      "Epoch [702/1000], Loss: 0.3130, accuracy: 0.9120\n",
      "Epoch [703/1000], Loss: 0.3130, accuracy: 0.9119\n",
      "Epoch [704/1000], Loss: 0.3129, accuracy: 0.9120\n",
      "Epoch [705/1000], Loss: 0.3129, accuracy: 0.9120\n",
      "Epoch [706/1000], Loss: 0.3129, accuracy: 0.9119\n",
      "Epoch [707/1000], Loss: 0.3128, accuracy: 0.9118\n",
      "Epoch [708/1000], Loss: 0.3128, accuracy: 0.9119\n",
      "Epoch [709/1000], Loss: 0.3128, accuracy: 0.9118\n",
      "Epoch [710/1000], Loss: 0.3127, accuracy: 0.9120\n",
      "Epoch [711/1000], Loss: 0.3127, accuracy: 0.9120\n",
      "Epoch [712/1000], Loss: 0.3126, accuracy: 0.9119\n",
      "Epoch [713/1000], Loss: 0.3126, accuracy: 0.9118\n",
      "Epoch [714/1000], Loss: 0.3126, accuracy: 0.9119\n",
      "Epoch [715/1000], Loss: 0.3125, accuracy: 0.9120\n",
      "Epoch [716/1000], Loss: 0.3125, accuracy: 0.9120\n",
      "Epoch [717/1000], Loss: 0.3125, accuracy: 0.9120\n",
      "Epoch [718/1000], Loss: 0.3124, accuracy: 0.9119\n",
      "Epoch [719/1000], Loss: 0.3124, accuracy: 0.9119\n",
      "Epoch [720/1000], Loss: 0.3124, accuracy: 0.9118\n",
      "Epoch [721/1000], Loss: 0.3123, accuracy: 0.9120\n",
      "Epoch [722/1000], Loss: 0.3123, accuracy: 0.9120\n",
      "Epoch [723/1000], Loss: 0.3122, accuracy: 0.9121\n",
      "Epoch [724/1000], Loss: 0.3122, accuracy: 0.9118\n",
      "Epoch [725/1000], Loss: 0.3122, accuracy: 0.9119\n",
      "Epoch [726/1000], Loss: 0.3121, accuracy: 0.9119\n",
      "Epoch [727/1000], Loss: 0.3121, accuracy: 0.9120\n",
      "Epoch [728/1000], Loss: 0.3121, accuracy: 0.9119\n",
      "Epoch [729/1000], Loss: 0.3120, accuracy: 0.9122\n",
      "Epoch [730/1000], Loss: 0.3120, accuracy: 0.9118\n",
      "Epoch [731/1000], Loss: 0.3120, accuracy: 0.9118\n",
      "Epoch [732/1000], Loss: 0.3119, accuracy: 0.9121\n",
      "Epoch [733/1000], Loss: 0.3119, accuracy: 0.9121\n",
      "Epoch [734/1000], Loss: 0.3118, accuracy: 0.9120\n",
      "Epoch [735/1000], Loss: 0.3118, accuracy: 0.9120\n",
      "Epoch [736/1000], Loss: 0.3118, accuracy: 0.9120\n",
      "Epoch [737/1000], Loss: 0.3117, accuracy: 0.9121\n",
      "Epoch [738/1000], Loss: 0.3117, accuracy: 0.9121\n",
      "Epoch [739/1000], Loss: 0.3117, accuracy: 0.9120\n",
      "Epoch [740/1000], Loss: 0.3116, accuracy: 0.9122\n",
      "Epoch [741/1000], Loss: 0.3116, accuracy: 0.9121\n",
      "Epoch [742/1000], Loss: 0.3116, accuracy: 0.9123\n",
      "Epoch [743/1000], Loss: 0.3115, accuracy: 0.9122\n",
      "Epoch [744/1000], Loss: 0.3115, accuracy: 0.9122\n",
      "Epoch [745/1000], Loss: 0.3115, accuracy: 0.9122\n",
      "Epoch [746/1000], Loss: 0.3114, accuracy: 0.9123\n",
      "Epoch [747/1000], Loss: 0.3114, accuracy: 0.9123\n",
      "Epoch [748/1000], Loss: 0.3114, accuracy: 0.9123\n",
      "Epoch [749/1000], Loss: 0.3113, accuracy: 0.9123\n",
      "Epoch [750/1000], Loss: 0.3113, accuracy: 0.9123\n",
      "Epoch [751/1000], Loss: 0.3113, accuracy: 0.9123\n",
      "Epoch [752/1000], Loss: 0.3112, accuracy: 0.9123\n",
      "Epoch [753/1000], Loss: 0.3112, accuracy: 0.9124\n",
      "Epoch [754/1000], Loss: 0.3112, accuracy: 0.9124\n",
      "Epoch [755/1000], Loss: 0.3111, accuracy: 0.9126\n",
      "Epoch [756/1000], Loss: 0.3111, accuracy: 0.9125\n",
      "Epoch [757/1000], Loss: 0.3111, accuracy: 0.9123\n",
      "Epoch [758/1000], Loss: 0.3110, accuracy: 0.9123\n",
      "Epoch [759/1000], Loss: 0.3110, accuracy: 0.9123\n",
      "Epoch [760/1000], Loss: 0.3109, accuracy: 0.9123\n",
      "Epoch [761/1000], Loss: 0.3109, accuracy: 0.9123\n",
      "Epoch [762/1000], Loss: 0.3109, accuracy: 0.9126\n",
      "Epoch [763/1000], Loss: 0.3108, accuracy: 0.9125\n",
      "Epoch [764/1000], Loss: 0.3108, accuracy: 0.9125\n",
      "Epoch [765/1000], Loss: 0.3108, accuracy: 0.9126\n",
      "Epoch [766/1000], Loss: 0.3107, accuracy: 0.9126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-54cc004f6438>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-d572a8be34d9>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(epochs, model, loss_fn, optim, train_dl, val_dl, metric)\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-e9d56326e43c>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, loss_fn, val_dataset, metric)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Finds loss for validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-e9d56326e43c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Finds loss for validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Files\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Files\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Files\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Files\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Files\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Files\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Files\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(1000, model, F.cross_entropy, optimizer, train_dataset, val_dataset, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=MNIST(root='data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label=test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To create batch from single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.unsqueeze(0).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict image for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of model on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2866, Accuracy: 0.9199\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=200)\n",
    "test_loss, total, test_acc = evaluate(model, loss_fn, test_loader, metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0147, -0.0165, -0.0281,  ..., -0.0011, -0.0135,  0.0079],\n",
       "                      [ 0.0129, -0.0159, -0.0236,  ...,  0.0258,  0.0183, -0.0188],\n",
       "                      [ 0.0168, -0.0178, -0.0118,  ..., -0.0014, -0.0181,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0317,  0.0196, -0.0253,  ...,  0.0189,  0.0002,  0.0031],\n",
       "                      [-0.0112,  0.0338,  0.0056,  ...,  0.0310,  0.0250, -0.0136],\n",
       "                      [ 0.0240, -0.0057,  0.0220,  ..., -0.0049,  0.0035, -0.0044]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.2657,  0.3364,  0.0793, -0.2274,  0.0151,  1.0448, -0.0877,  0.4937,\n",
       "                      -1.1849, -0.1946]))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0147, -0.0165, -0.0281,  ..., -0.0011, -0.0135,  0.0079],\n",
       "                      [ 0.0129, -0.0159, -0.0236,  ...,  0.0258,  0.0183, -0.0188],\n",
       "                      [ 0.0168, -0.0178, -0.0118,  ..., -0.0014, -0.0181,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0317,  0.0196, -0.0253,  ...,  0.0189,  0.0002,  0.0031],\n",
       "                      [-0.0112,  0.0338,  0.0056,  ...,  0.0310,  0.0250, -0.0136],\n",
       "                      [ 0.0240, -0.0057,  0.0220,  ..., -0.0049,  0.0035, -0.0044]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.2657,  0.3364,  0.0793, -0.2274,  0.0151,  1.0448, -0.0877,  0.4937,\n",
       "                      -1.1849, -0.1946]))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Mnist_Model()\n",
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2866, Accuracy: 0.9199\n"
     ]
    }
   ],
   "source": [
    "test_loss, total, test_acc = evaluate(model2, loss_fn, test_loader, metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
